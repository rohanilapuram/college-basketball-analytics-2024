---
title: "CMDA-4654"
subtitle: "Project 1"
author: "Matthew Hill and Rohan Reddy Illapuram"
date: "April 1, 2024"
output:
  pdf_document:
    highlight: haddock
keep_tex: no
number_sections: no
html_document:
  df_print: paged
geometry: margin = 0.5in
header-includes:
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
- \usepackage{amsmath}
editor_options:
  chunk_output_type: inline
documentclass: article
urlcolor: blue
---
  
<!-- The above is set to automatically compile to a .pdf file.   -->
<!-- It will only succeed if LaTeX is installed. -->
  
<!-- If you absolutely can't get LaTeX installed and/or working, then you can compile to a .html first,  -->
<!-- by clicking on the arrow button next to knit and selecting Knit to HTML. -->

<!-- You must then print you .html file to a .pdf by using first opening it in a web browser and then printing to a .pdf -->


```{r setup, include=FALSE}
# This is the setup chunk
#  Here you can set global options for the entire document

library(knitr) # I recommend doing this here
library(rvest)
library(dplyr)
library(tidyr)
library(tm)
library(car)
library(lmtest)
library(GGally)
library(MASS)
library(leaps)

# Although you can call functions from a library using the following notation
#  without loading the entire library.
knitr::opts_chunk$set(echo = TRUE, 
                      comment = NA, # Required
                      fig.path = "./figures/",  # Store all figures here in relative path (make the folder first)
                      fig.align = "center",
                      fig.width = 7,
                      fig.height = 7,
                      message = FALSE, # Turn off load messages
                      warning = FALSE # Turn off warnings
                      )

```

\clearpage

```{r include=FALSE}
# You should not echo this chunk.
# include=FALSE does more than echo=FALSE, it actually does: echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'

# You should set your working directory at the very beginning of your R Markdown file
#setwd("/Users/matthewhill/Documents/VT/Junior/CMDA 4654/Project 1/P1_CMDA4654")

# In linux ~/ is shorthand for /home/username/
# You should type things out properly for your system
# Mac: /Users/matthewhill/Documents/VT/Junior/CMDA 4654/
# Windows: C:/Users/username/Documents/etc/Lecture/Lecture_03/.../


```

<!-- ---------------------------------------------------------------------------------------------------- -->
<!-- ---------------- Homework Problems start below these lines ----------------------------------------- -->
<!-- ---------------------------------------------------------------------------------------------------- -->


### Web Scraping Data and Cleaning Data
```{r}
link = "https://kenpom.com/"
page = read_html(link)
table = page %>% html_nodes("table#ratings-table") %>%
  html_table() %>% .[[1]]
cbb_df = data.frame(table)

new_column_names = cbb_df[1, ]
names(cbb_df) = new_column_names
cbb_df = cbb_df[-1, ]

cbb_df = cbb_df[-c(7,9,11,13,15,17,19,21)]
rownames(cbb_df) = NULL
cbb_df = cbb_df[-c(41,42,83,84,125,126,167,168,209,210,251,252,293,294,335,336,377,378), ]
rownames(cbb_df) = cbb_df$Rk

cbb_df = separate(cbb_df, "W-L", into = c("Wins", "Losses"), sep = "-")

cbb_df$Rk = as.numeric(cbb_df$Rk)
cbb_df$Team = removeNumbers(cbb_df$Team)
cbb_df$Wins = as.numeric(cbb_df$Wins)
cbb_df$Losses = as.numeric(cbb_df$Losses)
cbb_df$AdjEM = as.numeric(gsub("\\+", "", cbb_df$AdjEM))
cbb_df$AdjO = as.numeric(cbb_df$AdjO)
cbb_df$AdjD = as.numeric(cbb_df$AdjD)
cbb_df$AdjT = as.numeric(cbb_df$AdjT)
cbb_df$Luck = as.numeric(gsub("\\+", "", cbb_df$Luck))
cbb_df$AdjEM.1 = as.numeric(gsub("\\+", "", cbb_df$AdjEM.1))
cbb_df$OppO = as.numeric(cbb_df$OppO)
cbb_df$OppD = as.numeric(cbb_df$OppD)
cbb_df$AdjEM.2 = as.numeric(gsub("\\+", "", cbb_df$AdjEM.2))

names(cbb_df)[names(cbb_df) == "AdjEM"] = "EM"
names(cbb_df)[names(cbb_df) == "AdjO"] = "OE"
names(cbb_df)[names(cbb_df) == "AdjD"] = "DE"
names(cbb_df)[names(cbb_df) == "AdjT"] = "Tempo"
names(cbb_df)[names(cbb_df) == "AdjEM.1"] = "SOS"
names(cbb_df)[names(cbb_df) == "AdjEM.2"] = "NCSOS"
```

### Fitting a MLR 
```{r}
## Full Model with all variables
power_5 = c("ACC", "Big 12", "Big Ten", "Pac-12", "SEC")
predictive_data = cbb_df %>% filter(Conf %in% power_5)
head(predictive_data)
predictive_data = predictive_data[ ,-c(2,3)]

full_mdl = lm(Wins ~ ., data = predictive_data)
summary(full_mdl)
vif(full_mdl)

## Reduced model to eliminate multicollinearity
reduced_mdl = lm(Wins ~ OE + DE + Tempo + Luck + OppO + OppD + NCSOS, data = predictive_data)
summary(reduced_mdl)
vif(reduced_mdl)
avPlots(reduced_mdl)

## Assumptions
student_r = rstudent(reduced_mdl)
fitted_values = reduced_mdl$fitted.values

plot(fitted_values, student_r, xlab = 'Fitted Values', ylab = 'Studentized Residuals')
abline(0,0, col = 'blue')
bptest(reduced_mdl)

qqnorm(student_r)
abline(0,1,col='red')
hist(student_r)
shapiro.test(student_r)

## Outliers
plot(fitted_values, student_r, xlab = 'Fitted Values', ylab = 'Studentized Residuals', main = 'Student vs Fitted')
abline(0,0, col = 'blue')
abline(h = c(-2,2), col= 'red')

h = hatvalues(reduced_mdl)
plot(h, main = 'Leverage Plots', ylab = 'Leverage Values', xlab = 'Observation Index')
p = 4; n = length(predictive_data[,1])
cutoff = (2*p)/n
abline(h = cutoff, col = 'black')

cd = cooks.distance(reduced_mdl)
plot(cd, ylab = 'Cooks Distance', xlab = 'Observation Index', main = 'Cooks Distance Plot')
abline(h = 0.1, col = 'black')
predictive_data[which(cd > 0.6), ]
cbb_df[43, ]
```

### Exhaustive Model Approach
```{r}
regit.full = regsubsets(Wins ~ OE + DE + Tempo + Luck + OppO + OppD + NCSOS, data = predictive_data, method = 'exhaustive', nbest = 1)
output = summary(regit.full, all.best = TRUE)
criterion_mat = cbind(output$rsq, output$adjr2, output$cp, output$bic)
colnames(criterion_mat) = c('R2', 'AdjR2', 'Cp', 'BIC')
results_mat = cbind(output$outmat, round(criterion_mat, 3))
results_mat
```

### Analyzing what was found to be the best model
```{r}
best_mdl = lm(Wins ~ OE + DE + Tempo + Luck + OppD + NCSOS, data = predictive_data)
summary(best_mdl)
vif(best_mdl)
avPlots(best_mdl)

## Assumptions
student_r = rstudent(best_mdl)
fitted_values = best_mdl$fitted.values

plot(fitted_values, student_r, xlab = 'Fitted Values', ylab = 'Studentized Residuals')
abline(0,0, col = 'blue')
bptest(best_mdl)

qqnorm(student_r)
abline(0,1,col='red')
hist(student_r)
shapiro.test(student_r)
```


### Ridge regression

Ridge regression is a regularization technique(Method in statistics used to reduce error caused by overfitting of data) for linear regression models. Used to get rid of overfitting in training data we use for our model. It is also know as L2 regularization. Problem that is solved using this regression is "Multicollinearity". In this technique of regilarization we add a bias into the model for decreasing model's variance. 

Residual Sum Squares formula for linear regression is given by \ \
$RSS = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$ \
\
Where: \
n is the number of data points in the dataset. \
$y_i$ is the observed value of the dependent variable for data point \
$\hat{y}_i$ is the predicted value of the dependent variable for data point i based on the regression model. \ \
Where as by adding the regularization term according to Ridge regression we would get \ \
$RSS_{ridge} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2$
$\lambda$ is the regularization parameter (also known as the ridge parameter or penalty parameter) that controls the strength of the regularization.} \
$p$ is the number of predictor variables (features) in the regression model.\
$\beta_j$ represents the coefficients (weights) associated with each predictor variable. \





```{r}
library(ggplot2)
library(reshape2)  # For melt() function

# Assuming 'cbb_df' is your data frame containing the data
# Extract the relevant columns for correlation analysis
cols_of_interest <- c("Wins", "Losses", "EM", "OE", "DE", "Tempo", "Luck", "SOS", "OppO", "OppD", "NCSOS")
data_subset <- cbb_df[, cols_of_interest]

# Calculate the correlation matrix
correlation_matrix <- cor(data_subset)

# Convert the correlation matrix to long format for ggplot
correlation_data <- melt(correlation_matrix)

# Create the correlation heatmap using ggplot2
heatmap <- ggplot(correlation_data, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), color = "black", size = 4) +
  scale_fill_gradient(low = "white", high = "red", na.value = "grey50", name = "Correlation") +
  labs(title = "Correlation Heatmap", x = NULL, y = NULL) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom",
    legend.direction = "horizontal",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank()
  )

heatmap

```

Research Question: 

```{r}
# Load the necessary libraries
library(glmnet)
library(dplyr)
library(plotly)  # for 3D plots
 
# Split the data into predictors (X) and response variable (y)
X <- as.matrix(cbb_df[, c("OE", "SOS")])
y <- cbb_df$Wins

# Split the data into training and testing sets (e.g., 70% training, 30% testing)
set.seed(123)  # for reproducibility
train_indices <- sample(nrow(cbb_df), 0.7 * nrow(cbb_df))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[-train_indices, ]
y_test <- y[-train_indices]

# Perform linear regression on the training data

ridge_model <- cv.glmnet(x = X_train, y = y_train, alpha = 0)  # alpha = 0 for ridge regression

```

## Making predictions using ridge model
```{r}
# Get the optimal lambda value chosen by cross-validation
optimal_lambda <- ridge_model$lambda.min  # You can also use `lambda.1se` for a more regularized model

# Extract the coefficients for the optimal lambda value
coefficients_ridge <- coef(ridge_model, s = optimal_lambda)

# Form the equation of the ridge regression model
intercept <- coefficients_ridge[1]
coefficients <- coefficients_ridge[-1]  # Exclude the intercept term
features <- colnames(X_train)
ridge_equation <- paste("y =", paste(coefficients, features, sep = " * ", collapse = " + "), "+", intercept)

# Print the equation
print(ridge_equation)

# Define the plane equation coefficients
intercept <- -61.7454572
coef_OE <- 0.7459537
coef_SOS <- -0.2537968

# Define a function for the plane equation
plane_equation <- function(x, y) {
  return(intercept + coef_OE * x + coef_SOS * y)
}

# Calculate predicted wins using the plane equation
predicted_wins <- plane_equation(X_train[,"OE"], X_train[,"SOS"])

# Create a grid of points for the plane
grid_points <- expand.grid(OE = seq(min(X_train[,"OE"]), max(X_train[,"OE"]), length.out = 50),
                            SOS = seq(min(X_train[,"SOS"]), max(X_train[,"SOS"]), length.out = 50))
grid_points$Wins <- plane_equation(grid_points$OE, grid_points$SOS)

# Create the 3D scatter plot with plotly
ridge.plot <- plot_ly(x = X_train[,"OE"], y = X_train[,"SOS"], z = y_train, 
                      type = "scatter3d", mode = "markers", 
                      marker = list(color = "blue", size = 5)) %>%
  layout(title = "3D Scatter Plot of Training Data",
         scene = list(xaxis = list(title = "OE"), 
                      yaxis = list(title = "SOS"), 
                      zaxis = list(title = "Wins")))

# Add the plane using the grid points
ridge.plot <- add_surface(p = ridge.plot,
                          z = matrix(grid_points$Wins, 
                                      nrow = length(unique(grid_points$OE)), 
                                      ncol = length(unique(grid_points$SOS)), 
                                      byrow = TRUE),
                          x = unique(grid_points$OE),
                          y = unique(grid_points$SOS),
                          colorscale = "Viridis")

ridge.plot
z = matrix(grid_points$Wins, nrow = length(unique(grid_points$OE)), ncol = length(unique(grid_points$SOS)), byrow = TRUE)
length(z)

```

To obtain the equation of the ridge regression model, we first fitted the model using cross-validated ridge regression with the `cv.glmnet` function in R. This function selects an optimal lambda value through cross-validation. \

After fitting the ridge regression model, we extracted the coefficients corresponding to the optimal lambda value. The coefficients represent the weights assigned to each predictor variable in the model. \ 

The equation of the ridge regression model can be written as follows: \

\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n \] \

Where: \
- \( y \) is the dependent variable (e.g., Wins in our case). \
- \( \beta_0 \) is the intercept term. \
- \( \beta_1, \beta_2, \ldots, \beta_n \) are the coefficients corresponding to  predictor variables \( x_1, x_2, \ldots, x_n \) respectively. \

For our specific ridge regression model, the coefficients and variables are substituted into the equation to form the final equation, which can be written in the form: \

\[
\text{Wins} = -61.7454572 + 0.7459537 \times \text{OE} - 0.2537968 \times \text{SOS}
\]

```{r}
# Predictions from the linear regression model
lm_predictions <- predict(lm_model, newdata = as.data.frame(X_test))

# Predictions from the ridge regression model
ridge_predictions <- predict(ridge_model, newx = X_test, s = optimal_lambda)

# Calculate Mean Squared Error (MSE)
mse_lm <- mean((y_test - lm_predictions)^2)
mse_ridge <- mean((y_test - ridge_predictions)^2)

# Calculate R-squared (R2)
rsquared_lm <- 1 - (sum((y_test - lm_predictions)^2) / sum((y_test - mean(y_test))^2))
rsquared_ridge <- 1 - (sum((y_test - ridge_predictions)^2) / sum((y_test - mean(y_test))^2))

# Calculate Root Mean Squared Error (RMSE)
rmse_lm <- sqrt(mse_lm)
rmse_ridge <- sqrt(mse_ridge)

# Print the results
cat("Linear Regression Model:\n")
cat("MSE:", mse_lm, "\n")
cat("R-squared:", rsquared_lm, "\n")
cat("RMSE:", rmse_lm, "\n\n")

cat("Ridge Regression Model:\n")
cat("MSE:", mse_ridge, "\n")
cat("R-squared:", rsquared_ridge, "\n")
cat("RMSE:", rmse_ridge, "\n")

```



