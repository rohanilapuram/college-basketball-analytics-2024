---
title: "2024 College Basketball Analysis"
output:
  flexdashboard::flex_dashboard:
    orientation: columns  # Adjust orientation as needed
    theme: spacelab
    fontsize: 12pt
    
---


<style type="text/css">

li {font-size: 20px;}
.chart-title {  /* chart_title  */
   font-size: 18px;
   font-family: Algerian;
   
.highcharts-container { /* Change font size of content within Highcharts */
    font-size: 12px; /* Adjust the font size as needed */
  }
</style>

```{r setup, include=FALSE}
library(flexdashboard)
# This is the setup chunk
#  Here you can set global options for the entire document

library(knitr) # I recommend doing this here
library(rvest)
library(dplyr)
library(tidyr)
library(tm)
library(car)
library(lmtest)
library(GGally)
library(MASS)
library(leaps)


```




```{r, include=FALSE}

link = "https://kenpom.com/"
page = read_html(link)
table = page %>% html_nodes("table#ratings-table") %>%
  html_table() %>% .[[1]]
cbb_df = data.frame(table)

data = read.csv("CBBData.csv")
cbb_df2 = data.frame(data)
cbb_df2$NCAA_Tourney <- ifelse(grepl("NCAA", cbb_df2$School), "Yes", "No")
cbb_df2$School = gsub(" NCAA", "", cbb_df2$School)

names(cbb_df2)[names(cbb_df2) == "W.L."] = "Win_Loss_Percentage"
names(cbb_df2)[names(cbb_df2) == "W.1"] = "Conference Wins"
names(cbb_df2)[names(cbb_df2) == "L.1"] = "Conference Losses"
names(cbb_df2)[names(cbb_df2) == "W.2"] = "Home_W"
names(cbb_df2)[names(cbb_df2) == "L.2"] = "Home_L"
names(cbb_df2)[names(cbb_df2) == "W.3"] = "Away_W"
names(cbb_df2)[names(cbb_df2) == "L.3"] = "Away_L"
names(cbb_df2)[names(cbb_df2) == "Tm."] = "Points_For"
names(cbb_df2)[names(cbb_df2) == "Opp."] = "Points_Against"
names(cbb_df2)[names(cbb_df2) == "FG."] = "FG_Perecntage"
names(cbb_df2)[names(cbb_df2) == "X3P"] = "3P"
names(cbb_df2)[names(cbb_df2) == "X3PA"] = "3PA"
names(cbb_df2)[names(cbb_df2) == "X3P."] = "3P_Percentage"
names(cbb_df2)[names(cbb_df2) == "FT."] = "FT_Percentage"
ready_to_merge = cbb_df2[-c(1,3,4,5,7,8)]

ready_to_merge$`Conference Wins` = as.numeric(ready_to_merge$`Conference Wins`)
ready_to_merge$`Conference Losses` = as.numeric(ready_to_merge$`Conference Losses`)
ready_to_merge$Home_W = as.numeric(ready_to_merge$Home_W)
ready_to_merge$Home_L = as.numeric(ready_to_merge$Home_L)
ready_to_merge$Away_W = as.numeric(ready_to_merge$Away_W)
ready_to_merge$Away_L = as.numeric(ready_to_merge$Away_L)
ready_to_merge$Points_For = as.numeric(ready_to_merge$Points_For)
ready_to_merge$Points_Against = as.numeric(ready_to_merge$Points_Against)
ready_to_merge$MP = as.numeric(ready_to_merge$MP)
ready_to_merge$FG = as.numeric(ready_to_merge$FG)
ready_to_merge$FGA = as.numeric(ready_to_merge$FGA)
ready_to_merge$FG_Perecntage = as.numeric(ready_to_merge$FG_Perecntage)
ready_to_merge$`3P` = as.numeric(ready_to_merge$`3P`)
ready_to_merge$`3P_Percentage` = as.numeric(ready_to_merge$`3P_Percentage`)
ready_to_merge$FT = as.numeric(ready_to_merge$FT)
ready_to_merge$FTA = as.numeric(ready_to_merge$FTA)
ready_to_merge$FT_Percentage = as.numeric(ready_to_merge$FT_Percentage)
ready_to_merge$ORB = as.numeric(ready_to_merge$ORB)
ready_to_merge$TRB = as.numeric(ready_to_merge$TRB)
ready_to_merge$AST = as.numeric(ready_to_merge$AST)
ready_to_merge$STL = as.numeric(ready_to_merge$STL)
ready_to_merge$BLK = as.numeric(ready_to_merge$BLK)
ready_to_merge$TOV = as.numeric(ready_to_merge$TOV)
ready_to_merge$PF = as.numeric(ready_to_merge$PF)
ready_to_merge$NCAA_Tourney = as.factor(ready_to_merge$NCAA_Tourney)

new_column_names = cbb_df[1, ]
names(cbb_df) = new_column_names
cbb_df = cbb_df[-1, ]

cbb_df = cbb_df[-c(7,9,11,13,15,17,19,21)]
rownames(cbb_df) = NULL
cbb_df = cbb_df[-c(41,42,83,84,125,126,167,168,209,210,251,252,293,294,335,336,377,378), ]
rownames(cbb_df) = cbb_df$Rk

cbb_df = separate(cbb_df, "W-L", into = c("Wins", "Losses"), sep = "-")

cbb_df$Rk = as.numeric(cbb_df$Rk)
cbb_df$Team = removeNumbers(cbb_df$Team)
cbb_df$Wins = as.numeric(cbb_df$Wins)
cbb_df$Losses = as.numeric(cbb_df$Losses)
cbb_df$AdjEM = as.numeric(gsub("\\+", "", cbb_df$AdjEM))
cbb_df$AdjO = as.numeric(cbb_df$AdjO)
cbb_df$AdjD = as.numeric(cbb_df$AdjD)
cbb_df$AdjT = as.numeric(cbb_df$AdjT)
cbb_df$Luck = as.numeric(gsub("\\+", "", cbb_df$Luck))
cbb_df$AdjEM.1 = as.numeric(gsub("\\+", "", cbb_df$AdjEM.1))
cbb_df$OppO = as.numeric(cbb_df$OppO)
cbb_df$OppD = as.numeric(cbb_df$OppD)
cbb_df$AdjEM.2 = as.numeric(gsub("\\+", "", cbb_df$AdjEM.2))

names(cbb_df)[names(cbb_df) == "AdjEM"] = "EM"
names(cbb_df)[names(cbb_df) == "AdjO"] = "OE"
names(cbb_df)[names(cbb_df) == "AdjD"] = "DE"
names(cbb_df)[names(cbb_df) == "AdjT"] = "Tempo"
names(cbb_df)[names(cbb_df) == "AdjEM.1"] = "SOS"
names(cbb_df)[names(cbb_df) == "AdjEM.2"] = "NCSOS"
cbb_df$Team = trimws(cbb_df$Team)
ready_to_merge$School = trimws(ready_to_merge$School)
cbb_df$Team = gsub(" St\\.$", " State", cbb_df$Team)
ready_to_merge$School = gsub("-", " ", ready_to_merge$School)
ready_to_merge$School = gsub("Brigham Young", "BYU", ready_to_merge$School)
ready_to_merge$School = gsub("NC State", "N.C. State", ready_to_merge$School)
ready_to_merge$School = gsub("Southern Methodist", "SMU", ready_to_merge$School)
ready_to_merge$School = gsub("Nevada Las Vegas", "UNLV", ready_to_merge$School)
ready_to_merge$School = gsub("Virginia Commonwealth", "VCU", ready_to_merge$School)
ready_to_merge$School = gsub("Southern California", "USC", ready_to_merge$School)
ready_to_merge$School = gsub("Loyola", "Loyola Chicago", ready_to_merge$School)
ready_to_merge$School = gsub("Louisiana State", "LSU", ready_to_merge$School)
ready_to_merge$School = gsub("College of Charleston", "Charleston", ready_to_merge$School)
ready_to_merge$School = gsub("Sam Houston", "Sam Houston State", ready_to_merge$School)
ready_to_merge$School = gsub("Massachusetts Lowell", "UMass Lowell", ready_to_merge$School)
ready_to_merge$School = gsub("Texas A&M Corpus Christi", "Texas A&M Corpus Chris", ready_to_merge$School)
ready_to_merge$School = gsub("California Baptist", "Cal Baptist", ready_to_merge$School)
ready_to_merge$School = gsub("Pennsylvania", "Penn", ready_to_merge$School)
ready_to_merge$School = gsub("Kansas City", "UMKC", ready_to_merge$School)
ready_to_merge$School = gsub("Bowling Green State", "Bowling Green", ready_to_merge$School)
ready_to_merge$School = gsub("Southern Mississippi", "Southern Miss", ready_to_merge$School)
ready_to_merge$School = gsub("Grambling", "Grambling State", ready_to_merge$School)
ready_to_merge$School = gsub("Omaha", "Nebraska Omaha", ready_to_merge$School)
ready_to_merge$School = gsub("Central Connecticut State", "Central Connecticut", ready_to_merge$School)
ready_to_merge$School = gsub("Maryland Baltimore County", "UMBC", ready_to_merge$School)
ready_to_merge$School = gsub("Florida International", "FIU", ready_to_merge$School)
ready_to_merge$School = gsub("South Carolina Upstate", "USC Upstate", ready_to_merge$School)
ready_to_merge$School = gsub("FDU", "Fairleigh Dickinson", ready_to_merge$School)
ready_to_merge$School = gsub("Texas Rio Grande Valley", "UT Rio Grande Valley", ready_to_merge$School)
ready_to_merge$School = gsub("Prairie View", "Prairie View A&M", ready_to_merge$School)
ready_to_merge$School = gsub("Long Island University", "LIU", ready_to_merge$School)
ready_to_merge$School = gsub("Cal State Northridge", "Cal St. Northridge", ready_to_merge$School)
ready_to_merge$School = gsub("Cal State Fullerton", "Cal St. Fullerton", ready_to_merge$School)
ready_to_merge$School = gsub("Cal State Bakersfield", "Cal St. Bakersfield", ready_to_merge$School)
ready_to_merge$School = gsub("Loyola Chicago Marymount", "Loyola Marymount", ready_to_merge$School)
ready_to_merge$School = gsub("\\s*\\(([^\\)]+)\\)", " \\1", ready_to_merge$School)
ready_to_merge$School = gsub("Saint Mary's CA", "Saint Mary's", ready_to_merge$School)
ready_to_merge$School = gsub("St. John's NY", "St. John's", ready_to_merge$School)
ready_to_merge$School = gsub("Loyola Chicago IL", "Loyola Chicago", ready_to_merge$School)
ready_to_merge$School = gsub("Albany NY", "Albany", ready_to_merge$School)
ready_to_merge$School = gsub("Queens NC", "Queens", ready_to_merge$School)
ready_to_merge$School = gsub("Saint Francis PA", "Saint Francis", ready_to_merge$School)
ready_to_merge$School = gsub("Loyola Chicago MD", "Loyola MD", ready_to_merge$School)

merged_df = merge(cbb_df, ready_to_merge, by.x = "Team", by.y = "School", all.x = TRUE)
merged_df = merged_df[order(merged_df$Rk), ]
rownames(merged_df) = merged_df$Rk
merged_df[295, ]$`Conference Wins` = merged_df[295, ]$Wins
merged_df[295, ]$`Conference Losses` = merged_df[295, ]$Losses

```

# Dataset Description

Column {data-width=550}
-----------------------------------------------------------------------

The focus of our dashboard is to explore the interactions between multiple factors collected from NCAA Basketball teams this season. We gathered our data from two online sources. Our first data set was web scraped from ["kenpom.com"](https://kenpom.com/) and our second data set was downloaded from ["sports-reference.com"](https://www.sports-reference.com/cbb/schools/). We merged the two datasets on the name of the college in order to increase our number of variables, allowing us to have more avenues to go down when doing our analysis. Our final data set has 41 variables and 362 observations. 

```{r}
column_descriptions = c("Team Name",
                        "National Rank",
                        "Conference",
                        "Number of Games Won",
                        "Number of Games Lost",
                        "Efficiency Margin",
                        "Offensive Efficiency",
                        "Defensive Efficieny",
                        "Tempo (amount of possesions per game)",
                        "Luck Rating",
                        "Strength of Schedule",
                        "Opposition Offensive Efficiency",
                        "Opposition Defensive Efficiency",
                        "Non-Conference Strength of Schedule",
                        "Win-Loss Percentage",
                        "Number of Conference Games Won",
                        "Number of Conference Games Lost",
                        "Home Games Won",
                        "Home Games Lost", 
                        "Away Games Won",
                        "Away Games Lost",
                        "Total Points Scored",
                        "Total Points Given Up",
                        "Minutes Played",
                        "Field Goals Made",
                        "Field Goals Attempted",
                        "Field Goal Percentage",
                        "3-Pointers Made",
                        "3-Pointers Attempted",
                        "3-Pointer Percentage",
                        "Free-throws made",
                        "Free-throws Attempted", 
                        "Free-throw Percentage",
                        "Offensive Rebounds",
                        "Total Rebounds",
                        "Assists",
                        "Steals",
                        "Blocks",
                        "Turnovers",
                        "Personal Fouls",
                        "Made the NCAA Torunament"
                        )

df = data.frame(colnames(merged_df[1:41]), column_descriptions)
table = kable(df, col.names = c("Varaible", "Variable Description"))
table
```


# MLR

Column {data-width=550}
-----------------------------------------------------------------------

### Research Question
What factors contribute to the number of wins a team achieves in college basketball, and how accurately can a multiple linear regression (MLR) model predict the win count based on these factors?

#### Fitting a MLR with all predictors
```{r}
## Full Model with all variables
power_5 = c("ACC", "Big 12", "Big Ten", "Pac-12", "SEC")
predictive_data = cbb_df %>% filter(Conf %in% power_5)
head(predictive_data)
predictive_data = predictive_data[ ,-c(2,3)]

full_mdl = lm(Wins ~ ., data = predictive_data)
summary(full_mdl)
vif(full_mdl)
```

#### Reducing the model to elimate collinearity between predictors
```{r}
## Reduced model to eliminate multicollinearity
reduced_mdl = lm(Wins ~ OE + DE + Tempo + Luck + OppO + OppD + NCSOS, data = predictive_data)
summary(reduced_mdl)
vif(reduced_mdl)
avPlots(reduced_mdl)
```
\newline
From the Added-Variable Plots we can see that there is a linear relationship between almost all of the predictors and our response variable.

#### Checking the Assumptions of our model
```{r}
## Assumptions
student_r = rstudent(reduced_mdl)
fitted_values = reduced_mdl$fitted.values

plot(fitted_values, student_r, xlab = 'Fitted Values', ylab = 'Studentized Residuals')
abline(0,0, col = 'blue')
bptest(reduced_mdl)

qqnorm(student_r)
abline(0,1,col='red')
hist(student_r)
shapiro.test(student_r)
```

#### Outlier Analysis
```{r}
## Outliers
plot(fitted_values, student_r, xlab = 'Fitted Values', ylab = 'Studentized Residuals', main = 'Student vs Fitted')
abline(0,0, col = 'blue')
abline(h = c(-2,2), col= 'red')

h = hatvalues(reduced_mdl)
plot(h, main = 'Leverage Plots', ylab = 'Leverage Values', xlab = 'Observation Index')
p = 4; n = length(predictive_data[,1])
cutoff = (2*p)/n
abline(h = cutoff, col = 'black')

cd = cooks.distance(reduced_mdl)
plot(cd, ylab = 'Cooks Distance', xlab = 'Observation Index', main = 'Cooks Distance Plot')
abline(h = 0.1, col = 'black')
predictive_data[which(cd > 0.6), ]
cbb_df[45, ]
```
Interesting to see that N.C. State is an outlier. This is a team that went on a historic run to end the season, which could be causing them to become an influential point. Since they are not effecting our assumptions and are not an incorrect data point we will not do anything to remove them.

Column {data-width=450}
-----------------------------------------------------------------------

### Exhaustive Model Approach
```{r}
regit.full = regsubsets(Wins ~ OE + DE + Tempo + Luck + OppO + OppD + NCSOS, data = predictive_data, method = 'exhaustive', nbest = 1)
output = summary(regit.full, all.best = TRUE)
criterion_mat = cbind(output$rsq, output$adjr2, output$cp, output$bic)
colnames(criterion_mat) = c('R2', 'AdjR2', 'Cp', 'BIC')
results_mat = cbind(output$outmat, round(criterion_mat, 3))
results_mat
```

### Analyzing what was found to be the best model
```{r}
best_mdl = lm(Wins ~ OE + DE + Luck + NCSOS, data = predictive_data)
summary(best_mdl)
avPlots(best_mdl)

## Assumptions
student_r = rstudent(best_mdl)
fitted_values = best_mdl$fitted.values

plot(fitted_values, student_r, xlab = 'Fitted Values', ylab = 'Studentized Residuals')
abline(0,0, col = 'blue')
bptest(best_mdl)

qqnorm(student_r)
abline(0,1,col='red')
hist(student_r)
shapiro.test(student_r)
```
\newline
The model that we found to be the best at predicting the number of wins a team will get is: $$Wins = 12.20368 + 0.73x_{OE} - 0.75x_{DE} + 28.63x_{Luck} - -0.18x_{NCSOS}$$


# Ridge regression



Column {data-width=650}
-----------------------------------------------------------------------

### Research Question and What is ridge regression?

**Question**: What is the relation between Wins , SOS and OE, how accurately can predict Wins using SOS and OE by constructing a Ridge model? \

**Ridge Regression**

Ridge regression is a regularization technique(Method in statistics used to reduce error caused by overfitting of data) for linear regression models. Used to get rid of overfitting in training data we use for our model. It is also know as L2 regularization. Problem that is solved using this regression is "Multicollinearity". In this technique of regilarization we add a bias into the model for decreasing model's variance. 

Residual Sum Squares formula for linear regression is given by \ \
$RSS = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$ \
\
Where: \
n is the number of data points in the dataset. \
$y_i$ is the observed value of the dependent variable for data point \
$\hat{y}_i$ is the predicted value of the dependent variable for data point i based on the regression model. \ \
Where as by adding the regularization term according to Ridge regression we would get \ \
$RSS_{ridge} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2$
$\lambda$ is the regularization parameter (also known as the ridge parameter or penalty parameter) that controls the strength of the regularization.} \
$p$ is the number of predictor variables (features) in the regression model.\
$\beta_j$ represents the coefficients (weights) associated with each predictor variable. \


#### 3D Interactive plot of model.
```{r}
# Load the necessary libraries
library(glmnet)
library(dplyr)
library(plotly)  # for 3D plots
 
# Split the data into predictors (X) and response variable (y)
X <- as.matrix(cbb_df[, c("OE", "SOS")])
y <- cbb_df$Wins

# Split the data into training and testing sets (e.g., 70% training, 30% testing)
set.seed(123)  # for reproducibility
train_indices <- sample(nrow(cbb_df), 0.7 * nrow(cbb_df))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[-train_indices, ]
y_test <- y[-train_indices]



ridge_model <- cv.glmnet(x = X_train, y = y_train, alpha = 0)  # alpha = 0 for ridge regression

# Get the optimal lambda value chosen by cross-validation
optimal_lambda <- ridge_model$lambda.min  # You can also use `lambda.1se` for a more regularized model

# Extract the coefficients for the optimal lambda value
coefficients_ridge <- coef(ridge_model, s = optimal_lambda)

# Form the equation of the ridge regression model
intercept <- coefficients_ridge[1]
coefficients <- coefficients_ridge[-1]  # Exclude the intercept term
features <- colnames(X_train)
ridge_equation <- paste("y =", paste(coefficients, features, sep = " * ", collapse = " + "), "+", intercept)

# Print the equation
print(ridge_equation)

# Define the plane equation coefficients
intercept <- -61.7454572
coef_OE <- 0.7459537
coef_SOS <- -0.2537968

# Define a function for the plane equation
plane_equation <- function(x, y) {
  return(intercept + coef_OE * x + coef_SOS * y)
}

# Calculate predicted wins using the plane equation
predicted_wins <- plane_equation(X_train[,"OE"], X_train[,"SOS"])

# Create a grid of points for the plane
grid_points <- expand.grid(OE = seq(min(X_train[,"OE"]), max(X_train[,"OE"]), length.out = 50),
                            SOS = seq(min(X_train[,"SOS"]), max(X_train[,"SOS"]), length.out = 50))
grid_points$Wins <- plane_equation(grid_points$OE, grid_points$SOS)

# Create the 3D scatter plot with plotly
ridge.plot <- plot_ly(x = X_train[,"OE"], y = X_train[,"SOS"], z = y_train, 
                      type = "scatter3d", mode = "markers", 
                      marker = list(color = "blue", size = 5)) %>%
  layout(title = "3D Scatter Plot of Training Data",
         scene = list(xaxis = list(title = "OE"), 
                      yaxis = list(title = "SOS"), 
                      zaxis = list(title = "Wins")),
         width = 800, height = 600)

# Add the plane using the grid points
ridge.plot <- add_surface(p = ridge.plot,
                          z = matrix(grid_points$Wins, 
                                      nrow = length(unique(grid_points$OE)), 
                                      ncol = length(unique(grid_points$SOS)), 
                                      byrow = TRUE),
                          x = unique(grid_points$OE),
                          y = unique(grid_points$SOS),
                          colorscale = "Viridis")

ridge.plot
```




3D scatter plot of the training data, where the x-axis represents offensive efficiency (OE), the y-axis represents the strength of schedule (SOS), and the z-axis represents the number of wins (Wins). Each data point is represented as a marker in the plot,



Column {data-width=350}
-----------------------------------------------------------------------

### Making model equation for ridge regression

```{r}
summary(ridge_model)
```
To obtain the equation of the ridge regression model, we first fitted the model using cross-validated ridge regression with the `cv.glmnet` function in R. This function selects an optimal lambda value through cross-validation. \

After fitting the ridge regression model, we extracted the coefficients corresponding to the optimal lambda value. The coefficients represent the weights assigned to each predictor variable in the model. \ 

The equation of the ridge regression model can be written as follows: \

\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n \] \

Where: \
- \( y \) is the dependent variable (e.g., Wins in our case). \
- \( \beta_0 \) is the intercept term. \
- \( \beta_1, \beta_2, \ldots, \beta_n \) are the coefficients corresponding to  predictor variables \( x_1, x_2, \ldots, x_n \) respectively. \

For our specific ridge regression model, the coefficients and variables are substituted into the equation to form the final equation, which can be written in the form: \

\[
\text{Wins} = -61.34 + 0.75 \times \text{OE} - 0.27 \times \text{SOS}
\]

### R-squarted for this model

```{r}
ridge_predictions <- predict(ridge_model, newx = X_test, s = optimal_lambda)

# Calculate Mean Squared Error (MSE)
mse_ridge <- mean((y_test - ridge_predictions)^2)

# Calculate R-squared (R2)
rsquared_ridge <- 1 - (sum((y_test - ridge_predictions)^2) / sum((y_test - mean(y_test))^2))

# Calculate Root Mean Squared Error (RMSE)
rmse_ridge <- sqrt(mse_ridge)

# Print the results
cat("Ridge Regression Model:\n")
cat("MSE:", mse_ridge, "\n")
cat("R-squared:", rsquared_ridge, "\n")
cat("RMSE:", rmse_ridge, "\n")
```


These values suggest that the ridge regression model is moderately effective in predicting basketball team wins based on offensive efficiency and strength of schedule. However, there is still room for improvement as indicated by the non-zero MSE and RMSE values.
 
# LOESS Fit

Column {data-width=350}
-----------------------------------------------------------------------

### Research Question
What is the nature of the relationship between offensive efficiency (OE) and three-pointer percentage (3P%) in basketball, and how does the application of a loess fit compared to traditional linear regression enhance our understanding of this relationship? As the three-pointer percentage increases, offensive efficiency is expected to increase as well, as making three-pointers contributes more points per possession compared to two-point field goals. However, the relationship may not be strictly linear, and a loess fit may capture the non-linear patterns more accurately than a linear regression model.

### Fitting a simple linear model to our parameters
```{r}
lm_mdl = lm(OE ~ `3P_Percentage`, data = merged_df)

ggplot(merged_df, aes(x = `3P_Percentage`, y = OE)) + geom_point() +
  geom_smooth(method = "lm")

```


Column {data-width=650}
-----------------------------------------------------------------------

### Fitting an optimal LOESS Regression line with degree = 1

#### Plot 1
```{r}
ggplot(merged_df, aes(x = `3P_Percentage`, y = OE)) + geom_point() +
  geom_smooth(method = "loess", span = 0.4843714, se = F, method.args = list(degree = 1), color = "red")
```

#### Summary
```{r}
loess_fit = loess(merged_df$OE ~ merged_df$`3P_Percentage`, data = merged_df, span = 0.4843714, degree = 1)
results = summary(loess_fit)
results
```



### Fitting an optimal LOESS Regression line with degree = 2

#### Plot 2

```{r}
ggplot(merged_df, aes(x = `3P_Percentage`, y = OE)) + geom_point() +
  geom_smooth(method = "loess", span = 0.6612838, se = F, method.args = list(degree = 2), color = "blue")
```

#### Summary
```{r}
loess_fit = loess(merged_df$OE ~ merged_df$`3P_Percentage`, data = merged_df, span = 0.6612838, degree = 2)
results = summary(loess_fit)
results
```




# KNN Classification

Column {data-width=450}
-----------------------------------------------------------------------

### Research Question and PCA plot

**Question**: "How accurately can a K-nearest neighbors (KNN) classifier predict the success level of basketball teams based on their defensive efficiency (DE), strength of schedule (SOS), and tempo? \

The levels are divided based on Win-Loss Percentage as follows:

- "Successful": > 0.75
- "Above Average": 0.5 - 0.75
- "Average": 0.25 - 0.5
- "Below Average": < 0.25


We are performing $\textbf{k-Nearest Neighbors (kNN)}$ classification on a dataset with predictors $\textbf{"SOS" (Strength of Schedule)}$ and $\textbf{"DE" (Defensive Efficiency)}$, $\textbf{"Tempo"}$ categorizing teams based on their win-loss percentages into four categories: $\textbf{“Successful", “Above Average", “Average,"}$ and $\textbf{“Below Average"}$. It splits the data into training and test sets, trains a kNN classifier with $\textbf{k=7}$ neighbors, and evaluates its accuracy, providing insights into team categorization based on performance metrics.


```{r}
library(class)
library(caret)
library(RColorBrewer)
merged_df$team_cat <- ifelse(merged_df$Win_Loss_Percentage > 0.75, "Successful",
                              ifelse(merged_df$Win_Loss_Percentage > 0.5, "Above Average",
                                     ifelse(merged_df$Win_Loss_Percentage > 0.25, "Average",
                                            "Below Average")))
set.seed(123)  # For reproducibility
indices <- sample(1:nrow(merged_df), size = 0.8 * nrow(merged_df), replace = FALSE)

# Create training and test datasets
train_data <- merged_df[indices, ]
test_data <- merged_df[-indices, ]

X_train <- as.data.frame(train_data[, c("SOS", "DE", "Tempo" )])
y_train <- train_data$team_cat
X_test <- as.data.frame(test_data[, c("SOS", "DE", "Tempo")])
y_test <- test_data$team_cat

knitr::kable(head(merged_df[,c("SOS", "DE", "Tempo", "team_cat")]))

# Create KNN classifier
k <- 7  # Number of neighbors
knn_model <- knn(train = X_train, test = X_test, cl = y_train, k = k)
# Evaluate the model
accuracy <- mean(knn_model == y_test)
cat("Accuracy of KNN classifier:", accuracy, "\n")


```

#### PCA 

```{r}
pca_result <- prcomp(train_data[, c("SOS", "DE", "Tempo")], scale. = TRUE)

# Convert PCA results to a data frame
pca_df <- as.data.frame(pca_result$x)
pca_df$team_cat <- as.factor(y_train)
pca_df$Team <- train_data$Team  # Assuming "Team" column contains team names

# Create interactive PCA graph with plotly
pca_plot <- plot_ly(data = pca_df, x = ~PC1, y = ~PC2, color = ~team_cat, text = ~Team, type = "scatter", mode = "markers",
                    marker = list(size = 10, opacity = 0.8)) %>%
  layout(title = "Interactive PCA Graph of Features",
         xaxis = list(title = "Principal Component 1"),
         yaxis = list(title = "Principal Component 2"),
         showlegend = TRUE,
         hoverlabel = list(bgcolor = "white"))

# Show the interactive PCA graph
pca_plot

```

The above graph is obtained after performing Principal Component Analysis (PCA) on the $\textbf{SOS}$, $\textbf{DE}$, and $\textbf{Tempo}$ variables from the $\textbf{train_data}$. It converts the PCA results into a data frame and creates an interactive scatter plot using plotly, where each data point represents a team. The plot displays the teams in a two-dimensional space based on the first two principal components ($\textbf{PC1}$ and $\textbf{PC2}$), with color indicating the $\textbf{team_cat}$ variable (team category) and team names shown as hover text.



Column {data-width=350}
-----------------------------------------------------------------------

### K value vs Accuracies

#### Ploting Accuracy for various values of K


```{r}
k_values <- seq(1, 50, by = 1)

# Initialize a vector to store accuracies
accuracies <- numeric(length(k_values))

# Loop through each K value and train a KNN model
for (i in seq_along(k_values)) {
  knn_model <- knn(train = X_train, test = X_test, cl = y_train, k = k_values[i])
  accuracies[i] <- mean(knn_model == y_test)
}

# Create a data frame for plotting
accuracy_df <- data.frame(K = k_values, Accuracy = accuracies)

max_accuracy_index <- which.max(accuracy_df$Accuracy)

# Plot accuracies vs. K values with highlighted point
plot_ly(data = accuracy_df, x = ~K, y = ~Accuracy, type = "scatter", mode = "lines+markers") %>%
  add_markers(data = accuracy_df[max_accuracy_index, ], color = I("red"), size = 3) %>%
  layout(title = "Accuracies vs. K Values",
         xaxis = list(title = "K Value"),
         yaxis = list(title = "Accuracy"),
         hoverlabel = list(bgcolor = "white"))

```




Above plot is a comprehensive evaluation of KNN models with varying numbers of neighbors (K) ranging from 1 to 50. It calculates the accuracy of each KNN model by comparing its predictions on the test dataset against the actual labels.We identify the K value that achieves the highest accuracy and highlights this optimal point in the plot using a distinctive red color


### The Results

#### Confusion Matrix

```{r}

library(caret)
library(kableExtra)

library(gmodels)

# Assuming iris_knn_classes contains the predicted classes and test_classes contains the actual classes

# Create a data frame with the actual and predicted values
results <- data.frame(Actual = y_test, Predicted = knn_model)

# Create the confusion matrix using CrossTable
conf_matrix <- CrossTable(x = results$Predicted, y = results$Actual, prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)

# Print the confusion matrix
print(conf_matrix)

# Print the cross table


# Create a confusion matrix
conf_matrix <- confusionMatrix(data = factor(results$Predicted, levels = unique(y_test)),
                               reference = factor(y_test, levels = unique(y_test)))
conf_matrix


```

The cross table compares the actual classes with the predicted classes from a classification model. It contains counts of correct predictions for each class combination: \
\
- "Above Average": Correctly predicted 33 times out of 42 instances (78.57% accuracy). \
- "Average": Correctly predicted 20 times out of 31 instances (64.52% accuracy). \
- "Below Average": Correctly predicted 4 times out of 4 instances (100% accuracy). \
- "Successful": Correctly predicted 0 times out of 2 instances (0% accuracy). \
The total number of instances considered in the table is 73


# Naive Bayes Classification

Column {data-width=350}
-----------------------------------------------------------------------

### Research Question
How can Naive Bayes classification be utilized to categorize college basketball teams as good, average, or bad 3-point shooting teams based on their three-pointer percentage, considering that we observed a positive relationship between three-pointer percentage and Offensive Efficiency when using LOESS?


### Distribution of 3-Point Percentage Categories
```{r}
excellent_threshold <- 0.40
good_threshold <- 0.35
average_threshold <- 0.30

merged_df$`3P_Shooting_Category` <- cut(merged_df$`3P_Percentage`,
                                      breaks = c(0, average_threshold, good_threshold, excellent_threshold, Inf),
                                      labels = c("poor", "average", "good", "excellent"),
                                      include.lowest = TRUE)

category_counts = table(merged_df$`3P_Shooting_Category`)
category_df = data.frame(Category = names(category_counts), Count = as.numeric(category_counts))


ggplot(category_df, aes(x = category_df$Category, y = category_df$Count, fill = Category)) +
  geom_bar(stat = "identity") +
  labs(title = "Distribution of 3-Point Shooting Categories",
       x = "Category", y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Column {data-width = 650}
-----------------------------------------------------------------------

### Naive Bayes Classification
```{r}
library(e1071)

train_index <- sample(1:nrow(merged_df), 0.8 * nrow(merged_df))  # 80% for training
train_data <- merged_df[train_index, ]
test_data <- merged_df[-train_index, ]
train_y = merged_df[train_index, ]$`3P_Shooting_Category`
test_y = merged_df[-train_index, ]$`3P_Shooting_Category`


# Train the Naive Bayes classifier
nb_classifier <- naiveBayes(`3P_Shooting_Category` ~ OE + Tempo + Points_For + FG_Perecntage, data = train_data)

# Predict on the testing set
predictions <- predict(nb_classifier, test_data)

# Calculate accuracy
accuracy <- mean(predictions == test_data$`3P_Shooting_Category`)
cat("Accuracy:", accuracy, "\n")

yhat <- predict(nb_classifier, test_data)
```


#### Confusion Matrix
```{r}
library(gmodels)
CrossTable(yhat, test_y,
           prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
           dnn = c("predicted", "actual"))
```


#### PCA Plot
```{r}
is_correct <- function(actual, predicted) {
  return(actual == predicted)
}

# Apply the function to each row in the test data
test_data$correct_classification <- mapply(is_correct, test_data$`3P_Shooting_Category`, predictions)

# Perform PCA
pca_data <- prcomp(train_data[, c("OE", "Tempo", "Points_For", "FG_Perecntage")], scale. = TRUE)

# Project test data onto PCA space
test_data_pca <- predict(pca_data, test_data[, c("OE", "Tempo", "Points_For", "FG_Perecntage")])

# Create a data frame for plotting
plot_df <- data.frame(PC1 = test_data_pca[, "PC1"],
                      PC2 = test_data_pca[, "PC2"],
                      Category = test_data$`3P_Shooting_Category`,
                      Correctly_Classified = test_data$correct_classification)

# Plot
ggplot(plot_df, aes(x = PC1, y = PC2, color = Category, shape = Correctly_Classified)) +
  geom_point(size = 3) +
  scale_color_manual(values = c("red", "blue", "green", "purple")) +
  scale_shape_manual(values = c(16, 18)) +
  labs(title = "PCA Plot with Classification Results",
       x = "PC1", y = "PC2", color = "Category",
       shape = "Classification") +
  theme_minimal()
```



# Logistic Regression

Column {data-width=650}
-----------------------------------------------------------------------

### Research Question


What is the relationship between a team's number of wins (Wins) and their likelihood of having an above-average 3-point percentage (3P_Percentage > mean) versus a below-average 3-point percentage (3P_Percentage <= mean)? \

**Variables Used**

Let’s briefly look at the data


- *Wins*: The number of wins achieved by a team, likely indicating their overall performance or success in games.
- *3P_Percentage*: The percentage of successful three-point shots made by a team, a measure of their accuracy and skill in long-range shooting.
- *Binary_3P*: A binary variable indicating whether a team's 3P_Percentage is above the mean (1 for "High 3P") or below/equal to the mean (0 for "Low/Medium 3P"), used as the target variable for logistic regression prediction.

#### First 6 rows of the dataset


```{r}
mean_3p_percentage <- mean(merged_df$'3P_Percentage')

# Categorize based on the mean
merged_df$Binary_3P <- ifelse(merged_df$'3P_Percentage' > mean_3p_percentage, 1, 0)



selected_vars <- c("Team", "Rk", "Wins", "3P_Percentage", "Binary_3P")
selected_data <- merged_df[selected_vars]

# Printing the head of the selected data
knitr::kable(head(selected_data))

```

```{r}


# Split the data into training and test sets
set.seed(42)
train_indices <-  sample(1:nrow(merged_df), size = 0.8 * nrow(merged_df), replace = FALSE)
train_data <- merged_df[train_indices, ]
test_data <- merged_df[-train_indices, ]



# Fit logistic regression model
log_model <- glm(Binary_3P ~ Wins, data = train_data, family = binomial)



# Make predictions on the test set
test_data$Predicted_Probabilities <- predict(log_model, newdata = test_data, type = "response")

# Convert predicted probabilities to predicted classes (0 or 1)
test_data$Predicted_Classes <- ifelse(test_data$Predicted_Probabilities > 0.5, 1, 0)

# Calculate accuracy
accuracy <- mean(test_data$Predicted_Classes == test_data$Binary_3P)

# Print accuracy
#cat("Accuracy of Logistic Regression model:", accuracy, "\n")

# Fit logistic regression model
log_model <- glm(Binary_3P ~ Wins, data = train_data, family = binomial)

# Create a sequence of Wins values for prediction
wins_seq <- seq(min(train_data$Wins), max(train_data$Wins), length.out = 100)

# Predict probabilities for the Wins sequence
predicted_probs <- predict(log_model, newdata = data.frame(Wins = wins_seq), type = "response")

# Plot the logistic regression curve
ggplot(merged_df, aes(x = Wins, y = Binary_3P)) +
  geom_point(aes(color = Binary_3P), position = position_jitter(height = 0.09, width = 0)) +
  geom_smooth(method = "glm", method.args = list(family="binomial")) +
  labs(y = "P(High 3P)", title = "Logistic Regression Curve") +
  theme_bw()


```



Column {data-width=350}
-----------------------------------------------------------------------

### Summary

#### Summary of our model is:

```{r}
summary(log_model)

```


The logistic regression equation for the model is:

\[
\eta = -3.4 + 0.20 \times \text{Wins}
\]

Where:
- \( \eta \) (eta) is the linear predictor.
- `Wins` is the predictor variable.
- The intercept is -3.34966.
- The coefficient for `Wins` is 0.20159.


```{r}
# Fit the reduced model (with fewer predictors)
reduced_model <- glm(Binary_3P ~ 1, data = train_data, family = binomial)

# Fit the full model (with more predictors)
full_model <- glm(Binary_3P ~ Wins, data = train_data, family = binomial)

# Perform drop-in-deviance LR test
lr_test <- anova(reduced_model, full_model, test = "Chisq")

# Print the LR test results
print(lr_test)


```

#### Drop-in-Deviance Likelihood Ratio Test

Assessing the overall goodness-of-fit.

```{r}

# Fit the reduced model (with fewer predictors)
reduced_model <- glm(Binary_3P ~ 1, data = train_data, family = binomial)

# Fit the full model (with more predictors)
full_model <- glm(Binary_3P ~ Wins, data = train_data, family = binomial)

# Perform the likelihood ratio test (LR test)
lr_test <- anova(reduced_model, full_model, test = "LRT")

# Print the LR test results
print(lr_test)

```

the Analysis of Deviance Table suggests that including the predictor "Wins" significantly improves the logistic regression model's fit for predicting the binary outcome "Binary_3P." The model with "Wins" as a predictor explains more of the variability in the response variable compared to the null model 

### Accuracy

#### Confusion matrix 


```{r}

library(caret)
library(e1071)  # This library includes the confusionMatrix function

# Define the logistic regression model
log_model <- glm(Binary_3P ~ Wins, data = train_data, family = binomial)

# Predict probabilities on the test data
pihat_test <- predict(log_model, newdata = test_data, type = "response")

# Define the threshold
threshold <- 0.5

# Categorize predictions into 1 (Above Average) or 0 (Average) based on the threshold
predicted_category <- factor(ifelse(pihat_test > threshold, 1, 0))

# Create the confusion matrix
conf_matrix <- confusionMatrix(data = predicted_category, reference = factor(test_data$Binary_3P, levels = c(0, 1)))

# Print the confusion matrix
print(conf_matrix)




```

The confusion matrix and metrics like accuracy (63.01%) and Cohen's kappa (0.2582) reflect the binary classification model's performance. Sensitivity (60.00%) and specificity (65.79%) show its ability to detect positive and negative instances accurately. Positive predictive value (61.76%) and prevalence (47.95%) offer insights into prediction accuracy and dataset composition

```



